{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da15774-343d-42d9-8864-e0e0909ce5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab90d03b-0cdb-40dc-92b6-62d400a6d72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720fc73f0f584b87a252bf9c3b254a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac90673-8dcb-4e4f-acf0-4282c6a82cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "# --- 1. Model Setup ---\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Load model in 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map={\"\": \"cuda\"}\n",
    ")\n",
    "\n",
    "# Enable Gradient Checkpointing (Saves massive VRAM by recomputing parts of the graph)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Prepare model for k-bit training (stabilizes norms/layers)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9fdc57-32cf-4781-b97b-d30d64a60160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Apply Standard LoRA to Attention Layers ---\n",
    "# We keep this to adapt the attention mechanism alongside your MoE MLPs\n",
    "attn_lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, attn_lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9006ac-fe98-4209-8fec-0b497625fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Define MixLoRA Architecture ---\n",
    "\n",
    "class TopKRouter(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_experts, k=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_dim, num_experts)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, dim]\n",
    "        logits = self.linear(x)\n",
    "        router_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get top-k experts\n",
    "        # weights: [batch, seq, k], indices: [batch, seq, k]\n",
    "        weights, indices = router_probs.topk(self.k, dim=-1)\n",
    "        return weights, indices, router_probs\n",
    "\n",
    "class LoRAExpert(nn.Module):\n",
    "    \"\"\"\n",
    "    FIX: A true Bottleneck Adapter to save VRAM.\n",
    "    Structure: Linear(dim->r) -> SiLU -> Linear(r->dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, r=16, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Linear(hidden_dim, r, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "        self.lora_B = nn.Linear(r, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Result is [batch, seq, dim]\n",
    "        return self.lora_B(self.act(self.lora_A(self.dropout(x))))\n",
    "\n",
    "# --- Redefine this class with the casting fix ---\n",
    "class MixLoRAFFN_V2(nn.Module):\n",
    "    def __init__(self, base_ffn, hidden_dim, num_experts=8, k=2, r=16):\n",
    "        super().__init__()\n",
    "        self.base_ffn = base_ffn\n",
    "        self.router = TopKRouter(hidden_dim, num_experts, k)\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            LoRAExpert(hidden_dim, r=r) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        self.latest_router_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FIX: Force input to match the router's dtype (bfloat16)\n",
    "        # This fixes the mismatch when Gradient Checkpointing passes float32\n",
    "\n",
    "        x = x.to(self.router.linear.weight.dtype)\n",
    "\n",
    "        # 1. Compute Base Output (Frozen)\n",
    "        with torch.no_grad():\n",
    "            base_out = self.base_ffn(x)\n",
    "\n",
    "        # 2. Routing\n",
    "        weights, indices, router_probs = self.router(x)\n",
    "        self.latest_router_probs = router_probs\n",
    "\n",
    "        # 3. Compute Expert Mixture\n",
    "        expert_out = torch.zeros_like(base_out)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            is_selected = (indices == i).any(dim=-1, keepdim=True)\n",
    "\n",
    "            if is_selected.any():\n",
    "                expert_weight = (weights * (indices == i).float()).sum(dim=-1, keepdim=True)\n",
    "                current_expert_out = expert(x)\n",
    "                expert_out += is_selected * expert_weight * current_expert_out\n",
    "\n",
    "        return base_out + expert_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7762963f-28ce-457f-b2dd-d9d44850929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injecting MixLoRA layers...\n",
      "Detected Hidden Dimension: 2048\n",
      "Injection complete. Experts moved to GPU.\n"
     ]
    }
   ],
   "source": [
    " # --- 4. INJECT MixLoRA into the Model (FIXED) ---\n",
    "print(\"Injecting MixLoRA layers...\")\n",
    "\n",
    "HIDDEN_DIM = model.config.hidden_size\n",
    "print(f\"Detected Hidden Dimension: {HIDDEN_DIM}\")\n",
    "\n",
    "NUM_EXPERTS = 8\n",
    "TOP_K = 2\n",
    "\n",
    "# Helper to find layers\n",
    "def get_model_layers(model):\n",
    "    try: return model.base_model.model.model.layers\n",
    "    except AttributeError: return model.model.layers\n",
    "\n",
    "layers = get_model_layers(model)\n",
    "\n",
    "for layer in layers:\n",
    "    original_mlp = layer.mlp\n",
    "\n",
    "    # Create wrapper\n",
    "    mix_lora_layer = MixLoRAFFN_V2(\n",
    "        original_mlp,   # base_ffn\n",
    "        HIDDEN_DIM,     # hidden_dim\n",
    "        NUM_EXPERTS,    # num_experts\n",
    "        TOP_K,          # k\n",
    "        16 \n",
    "    )\n",
    "\n",
    "    # FIX: Move the new layer to GPU ('cuda') AND set to bfloat16\n",
    "    mix_lora_layer.to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "    # Ensure gradients are on\n",
    "    mix_lora_layer.train()\n",
    "    for param in mix_lora_layer.experts.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in mix_lora_layer.router.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    layer.mlp = mix_lora_layer\n",
    "    # After: layer.mlp = mix_lora_layer\n",
    "\n",
    "    for p in layer.mlp.router.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    for p in layer.mlp.experts.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "print(\"Injection complete. Experts moved to GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac7f57e-ddc2-4eb8-9e59-5c448b273e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # --- 5. Data Processing ---\n",
    "psy_dataset = load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    text = example[\"Context\"] + tokenizer.eos_token + example[\"Response\"]\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"labels\"] = [\n",
    "        -100 if t == tokenizer.pad_token_id else t\n",
    "        for t in tokens[\"input_ids\"]\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = psy_dataset.map(tokenize_fn, remove_columns=[\"Context\", \"Response\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=4, # Reduced batch size for safety\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922e5066-93f5-48bb-9c61-5c5811522745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "model.training = True\n",
      "Step 0: Loss 2.6937 | Aux Loss 10.0000\n",
      "Step 10: Loss 2.7693 | Aux Loss 10.0000\n",
      "Step 20: Loss 2.8881 | Aux Loss 10.0000\n",
      "Step 30: Loss 2.8520 | Aux Loss 10.0000\n",
      "Step 40: Loss 3.1615 | Aux Loss 10.0000\n",
      "Step 50: Loss 2.9552 | Aux Loss 10.0000\n",
      "Step 60: Loss 3.0282 | Aux Loss 10.0000\n",
      "Step 70: Loss 3.0262 | Aux Loss 10.0000\n",
      "Step 80: Loss 2.7985 | Aux Loss 10.0000\n",
      "Step 90: Loss 2.9236 | Aux Loss 10.0000\n",
      "Step 100: Loss 2.9167 | Aux Loss 10.0000\n",
      "Step 110: Loss 3.4290 | Aux Loss 10.0000\n",
      "Step 120: Loss 3.0029 | Aux Loss 10.0000\n",
      "Step 130: Loss 3.2111 | Aux Loss 10.0000\n",
      "Step 140: Loss 3.4685 | Aux Loss 10.0000\n",
      "Step 150: Loss 3.6568 | Aux Loss 10.0000\n",
      "Step 160: Loss 4.7006 | Aux Loss 10.0000\n",
      "Step 170: Loss 6.1372 | Aux Loss 10.0000\n",
      "Step 180: Loss 6.3940 | Aux Loss 10.0000\n",
      "Step 190: Loss 5.8888 | Aux Loss 10.0000\n",
      "Step 200: Loss 5.5446 | Aux Loss 10.0000\n",
      "Step 210: Loss 5.8338 | Aux Loss 10.0000\n",
      "Step 220: Loss 5.8722 | Aux Loss 10.0000\n",
      "Step 230: Loss 7.3577 | Aux Loss 10.0000\n",
      "Step 240: Loss 5.2188 | Aux Loss 10.0000\n",
      "Step 250: Loss 8.1126 | Aux Loss 10.0000\n",
      "Step 260: Loss 6.6976 | Aux Loss 10.0000\n",
      "Step 270: Loss 6.6555 | Aux Loss 10.0000\n",
      "Step 280: Loss 6.8472 | Aux Loss 10.0000\n",
      "Step 290: Loss 6.3867 | Aux Loss 10.0000\n",
      "Step 300: Loss 6.9774 | Aux Loss 10.0000\n",
      "Step 310: Loss 7.7134 | Aux Loss 10.0000\n",
      "Step 320: Loss 6.7117 | Aux Loss 10.0000\n",
      "Step 330: Loss 7.3299 | Aux Loss 10.0000\n",
      "Step 340: Loss 6.4182 | Aux Loss 10.0000\n",
      "Step 350: Loss 6.3191 | Aux Loss 10.0000\n",
      "Step 360: Loss 6.4176 | Aux Loss 10.0000\n",
      "Step 370: Loss 6.4921 | Aux Loss 10.0000\n",
      "Step 380: Loss 6.6182 | Aux Loss 10.0000\n",
      "Step 390: Loss 6.8604 | Aux Loss 10.0000\n",
      "Step 400: Loss 6.3900 | Aux Loss 10.0000\n",
      "Step 410: Loss 6.1960 | Aux Loss 10.0000\n",
      "Step 420: Loss 6.1143 | Aux Loss 10.0000\n",
      "Step 430: Loss 6.1754 | Aux Loss 10.0000\n",
      "Step 440: Loss 6.5067 | Aux Loss 10.0000\n",
      "Step 450: Loss 6.3800 | Aux Loss 10.0000\n",
      "Step 460: Loss 6.1385 | Aux Loss 10.0000\n",
      "Step 470: Loss 6.3593 | Aux Loss 10.0000\n",
      "Step 480: Loss 6.5673 | Aux Loss 10.0000\n",
      "Step 490: Loss 6.0550 | Aux Loss 10.0000\n",
      "Step 500: Loss 5.7910 | Aux Loss 10.0000\n",
      "Step 510: Loss 6.3975 | Aux Loss 10.0000\n",
      "Step 520: Loss 6.1729 | Aux Loss 10.0000\n",
      "Step 530: Loss 5.9953 | Aux Loss 10.0000\n",
      "Step 540: Loss 6.1933 | Aux Loss 10.0000\n",
      "Step 550: Loss 5.6819 | Aux Loss 10.0000\n",
      "Step 560: Loss 6.0504 | Aux Loss 10.0000\n",
      "Step 570: Loss 6.0556 | Aux Loss 10.0000\n",
      "Step 580: Loss 5.9571 | Aux Loss 10.0000\n",
      "Step 590: Loss 5.8567 | Aux Loss 10.0000\n",
      "Step 600: Loss 5.9103 | Aux Loss 10.0000\n",
      "Step 610: Loss 5.7637 | Aux Loss 10.0000\n",
      "Step 620: Loss 5.5840 | Aux Loss 10.0000\n",
      "Step 630: Loss 6.3127 | Aux Loss 10.0000\n",
      "Step 640: Loss 5.5204 | Aux Loss 10.0000\n",
      "Step 650: Loss 5.6336 | Aux Loss 10.0000\n",
      "Step 660: Loss 5.6006 | Aux Loss 10.0000\n",
      "Step 670: Loss 5.5901 | Aux Loss 10.0000\n",
      "Step 680: Loss 5.3212 | Aux Loss 10.0000\n",
      "Step 690: Loss 5.4724 | Aux Loss 10.0000\n",
      "Step 700: Loss 5.2830 | Aux Loss 10.0000\n",
      "Step 710: Loss 5.4033 | Aux Loss 10.0000\n",
      "Step 720: Loss 5.8264 | Aux Loss 10.0000\n",
      "Step 730: Loss 5.6020 | Aux Loss 10.0000\n",
      "Step 740: Loss 5.2450 | Aux Loss 10.0000\n",
      "Step 750: Loss 5.5897 | Aux Loss 10.0000\n",
      "Step 760: Loss 5.3608 | Aux Loss 10.0000\n",
      "Step 770: Loss 5.4225 | Aux Loss 10.0000\n",
      "Step 780: Loss 5.3904 | Aux Loss 10.0000\n",
      "Step 790: Loss 5.9284 | Aux Loss 10.0000\n",
      "Step 800: Loss 5.0309 | Aux Loss 10.0000\n",
      "Step 810: Loss 5.3296 | Aux Loss 10.0000\n",
      "Step 820: Loss 5.5721 | Aux Loss 10.0000\n",
      "Step 830: Loss 4.9380 | Aux Loss 10.0000\n",
      "Step 840: Loss 5.7722 | Aux Loss 10.0000\n",
      "Step 850: Loss 5.4356 | Aux Loss 10.0000\n",
      "Step 860: Loss 5.1152 | Aux Loss 10.0000\n",
      "Step 870: Loss 5.2724 | Aux Loss 10.0000\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Training Loop (Fixed Layer Access) ---\n",
    "\n",
    "# Re-define optimizer to be safe\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-4)\n",
    "\n",
    "# Load Balance Loss Function\n",
    "def load_balance_loss(router_probs):\n",
    "    # router_probs: [batch, seq, num_experts]\n",
    "    mean_prob = router_probs.mean(dim=(0, 1)) # Avg over batch and seq\n",
    "    return (mean_prob * mean_prob).sum() * num_experts\n",
    "\n",
    "num_experts = NUM_EXPERTS\n",
    "aux_loss_weight = 0.01\n",
    "\n",
    "model.train()\n",
    "print(\"Starting training...\")\n",
    "print(\"model.training =\", model.training)\n",
    "\n",
    "\n",
    "# Helper to find layers safely (Works for Peft + QLoRA)\n",
    "def get_layers_for_loss(model):\n",
    "    try:\n",
    "        # Standard QLoRA/Peft path\n",
    "        return model.base_model.model.model.layers\n",
    "    except AttributeError:\n",
    "        # Fallback\n",
    "        return model.model.layers\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        labels=batch[\"labels\"],\n",
    "    )\n",
    "\n",
    "    main_loss = outputs.loss\n",
    "\n",
    "    # FIX: Access layers safely for Aux Loss\n",
    "    aux_loss = torch.tensor(0.0, device=main_loss.device)\n",
    "\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, \"mlp\") and isinstance(layer.mlp, MixLoRAFFN_V2):\n",
    "            router_probs = layer.mlp.latest_router_probs\n",
    "    \n",
    "            if router_probs is not None:\n",
    "                # 1Ô∏è‚É£ Load-balance loss\n",
    "                aux_loss += load_balance_loss(router_probs)\n",
    "    \n",
    "                # 2Ô∏è‚É£ üî• Entropy regularization (ADD HERE)\n",
    "                entropy = -(\n",
    "                    router_probs * torch.log(router_probs + 1e-8)\n",
    "                ).sum(dim=-1).mean()\n",
    "    \n",
    "                aux_loss += 0.01 * entropy\n",
    "\n",
    "    aux_loss = torch.clamp(aux_loss, max=10.0)\n",
    "    total_loss = main_loss + (aux_loss_weight * aux_loss)\n",
    "\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "       print(f\"Step {step}: Loss {main_loss.item():.4f} | Aux Loss {aux_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0fc0c3-c3d6-4370-8450-3fc67e6c5deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>I'm not feeling good. What should I do? My friend is my parents, but he was the past with the same. This‚Äôs a few years and have a therapist and I am going to get me for my dad. I don't know it like the couple girl. I can't see the current man.   Do I feel that I did this about him. I think I have been an 12 disorder and are not never know to a past with her. But I had so afraid of the lot, but they do this. We're very sex? I‚Äôm been just almost normal and she's been much in anxiety.\n",
      " I don‚Äôt be a long life with a boyfriend and does he has been terrible. He doesn't never never have no way back and we have a history with our\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"I'm not feeling good. What should I do?\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "text = tokenizer.decode(outputs[0])\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e74f-68f5-435b-8400-32ed3e2590c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
